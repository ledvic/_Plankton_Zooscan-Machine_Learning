{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.feature import daisy, hog, ORB, local_binary_pattern, SIFT\n",
    "from skimage.color import label2rgb, rgb2gray\n",
    "from skimage.transform import resize, rotate, downscale_local_mean\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from skimage import exposure\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import psutil\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "\n",
    "import gabor_filters\n",
    "from  gabor_filters import gabor_filter\n",
    "from  gabor_filters import gabor_filter_response\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gabor_filters)\n",
    "importlib.reload(gabor_filters.gabor_filter)\n",
    "importlib.reload(gabor_filters.gabor_filter_response)\n",
    "\n",
    "from gabor_filters.gabor_filter import GaborFilterBank as gbb\n",
    "from gabor_filters.gabor_filter_response import GaborFilteredResponseBank as gbfrb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.16\n",
      "0.19.2\n"
     ]
    }
   ],
   "source": [
    "print(python_version())\n",
    "print(skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_from_filepath(filepath):\n",
    "    # read image from its path\n",
    "    img = io.imread(filepath, as_gray=True)\n",
    "    \n",
    "    # Create Gabor filter bank\n",
    "    fmax = 0.327 # maximum frequency\n",
    "    k = np.sqrt(2) #frequency ratio or factor for selecting filter frequencies\n",
    "    p = 0.5 # crossing point between two consecutive filters, default 0.5\n",
    "    u = 6 #number of frequencies\n",
    "    v = 8 #number of orientation\n",
    "    gamma = 0.5  #smoothting parameter \n",
    "    eta = 0.5  #smoothting parameter of\n",
    "    row = img.shape[0]\n",
    "    col = img.shape[1] # size of image\n",
    "\n",
    "    GaborFilterBank = gbb().create_a_set_of_gabor_filters(fmax, k, p, u, v, row, col, gamma, eta)\n",
    "    \n",
    "    # Filter with the filter bank\n",
    "    GaborFilteredReponses = gbfrb().create_a_set_of_Gabor_filtered_responses(img, GaborFilterBank)\n",
    "\n",
    "    # Convert responses to simple 3-D matrix with normalization\n",
    "    filteredImages = gbfrb().convert_a_set_Gabor_filtered_responses_to_ndarray(GaborFilteredReponses)\n",
    "    \n",
    "    # Get mean and standard deviation of each response as Gabor (texture) features of an input image\n",
    "    nImages = filteredImages.shape[2]\n",
    "    textureFeatures = np.zeros(nImages*2)\n",
    "\n",
    "    index=0\n",
    "    for i in range(0, nImages):\n",
    "        textureFeatures[index] = np.mean(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "        textureFeatures[index] = np.std(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "    \n",
    "    return textureFeatures\n",
    "\n",
    "def extract_gabor(dfDataset):\n",
    "    # chunk the large dataset int smaller pieces\n",
    "    n = 1000\n",
    "    list_dfDataset_chunk = [dfDataset[i:i+n] for i in range(0, len(dfDataset), n)]\n",
    "    \n",
    "    gabor_list = [None]*len(list_dfDataset_chunk)\n",
    "\n",
    "    with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
    "        with Parallel(n_jobs=10, require='sharedmem') as parallel:\n",
    "            for i, dfDataset_chunk in enumerate(tqdm(list_dfDataset_chunk, desc='Processing data', colour='blue', position=0, leave=True)):\n",
    "            \n",
    "                gabor_features = parallel(\n",
    "                                delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(dfDataset_chunk['filenames'], desc='Extract Gabor', colour='cyan', position=1, leave=False)\n",
    "                            )\n",
    "            \n",
    "                gabor_list[i] = gabor_features\n",
    "\n",
    "                del  gabor_features\n",
    "                gc.collect()\n",
    "    \n",
    "    return gabor_list\n",
    "\n",
    "\n",
    "def extract_gabor_2(dfDataset):\n",
    "    \n",
    "    # with parallel_backend(\"loky\", inner_max_num_threads=3):\n",
    "    with Parallel(n_jobs=30, require='sharedmem', return_generator=True) as parallel:\n",
    "        \n",
    "        gabor_features = parallel(\n",
    "                        delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(dfDataset['filenames'], desc='Extract Gabor', colour='cyan')\n",
    "                    )\n",
    "            \n",
    "    return gabor_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. main()\n",
    "\n",
    "### 4.2.1. For fold 1\n",
    "#### 1. Read path of fold 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFoldTraining_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-training-fold_1.csv')\n",
    "dfFoldValidation_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-validation-fold_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>labels</th>\n",
       "      <th>short_filenames</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0001-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0003-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0004-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0004.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filenames            labels  \\\n",
       "0  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "1  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "2  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "3  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "4  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "\n",
       "       short_filenames  cls  \n",
       "0  0001-aggregates.png    0  \n",
       "1             0002.png    0  \n",
       "2  0003-aggregates.png    0  \n",
       "3  0004-aggregates.png    0  \n",
       "4             0004.png    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(44099, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfFoldTraining_1.head(5), dfFoldTraining_1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting gabor feature for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b04b23baad2412db93060b347b10257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract Gabor:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_gabor_train = extract_gabor_2(dfFoldTraining_1[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gabor_train2 = np.vstack(list_gabor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 96)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_gabor_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_gabor_train2, list_gabor_train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "list_dfFoldTraining_1_chunked = [dfFoldTraining_1[i:i+n] for i in range(0, len(dfFoldTraining_1), n)]\n",
    "\n",
    "display(len(list_dfFoldTraining_1_chunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=2) as parallel:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabor_list_train = None\n",
    "\n",
    "pbar = tqdm(list_dfFoldTraining_1_chunked)\n",
    "\n",
    "for i, dfFoldTraining_1_chunked in enumerate(pbar):\n",
    "    pbar.set_description(f'Processing the chunked data {i+1}')\n",
    "    \n",
    "    gabor_list_train_chunked = extract_gabor(dfFoldTraining_1_chunked)\n",
    "\n",
    "    gabor_list_train = np.vstack(gabor_list_train_chunked)\n",
    "\n",
    "    del  gabor_list_train_chunked\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard deviation normalization for later uses\n",
    "train_std_norm = StandardScaler().fit(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation normalization\n",
    "HOG_list_std = train_std_norm.transform(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HOG_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total HOG features:',(HOG_list_std.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PCA analysis on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_HOG_std = PCA().fit(HOG_list_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Plot PCA components and CEV\n",
    "\n",
    "From this, we can know number of components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100, linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Cumulative explained variance');\n",
    "\n",
    "ax.set_yticks(np.arange(0,105,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.axhline(y=90, linewidth=3, color='g', alpha=0.5)\n",
    "\n",
    "# ax.plot(800, 91, marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"green\")\n",
    "\n",
    "ax.set_title(\"PCA analysis on HOG features of training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100)[1100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remarks: More than 90% of variance is explained by first 1100 components</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Kaiser's rule in statistics: Pick components which have eigenvalues >= 1 or 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(pca_HOG_std.explained_variance_, 'bo-', linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_yticks(np.arange(0,125,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Eigenvalue')\n",
    "plt.axhline(y=1, linewidth=1, color='g', alpha=0.5)\n",
    "plt.title('Scree Plot of PCA: Component Eigenvalues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEigenvalues \\n%s' %pca_HOG_std.explained_variance_)\n",
    "print('Eigenvectors \\n%s' %pca_HOG_std.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaiser's rule in statistics: Pick components which have eigenvalues >= 1 or 0.7\n",
    "a = pca_HOG_std.explained_variance_ >= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Only 309 components are significant and should be kept </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit PCA to the HOG features\n",
    "\n",
    "1. We fit PCA (with n_components to keep) onto the training set\n",
    "2. Transform the training set with that PCA\n",
    "3. Use that PCA to transform the validation & test set\n",
    "\n",
    "##### 4.1. For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 1100 components which contribute to > 90 %\n",
    "pca_HOG_std_2 = PCA(n_components=1100)\n",
    "pca_HOG_std_2.fit(HOG_list_std)\n",
    "HOG_PCA_train = pca_HOG_std_2.transform(HOG_list_std)\n",
    "print(\"Original shape:   \", HOG_list_std.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG for train set --- standardization again\n",
    "std_scale_train_2 = preprocessing.StandardScaler().fit(HOG_PCA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in file\n",
    "X_HOG_std_train = std_scale_train_2.transform(HOG_PCA_train)\n",
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_train)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldTraining_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-train-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the validation set\n",
    "HOG_validation_list = extract_hog(dfFoldValidation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization\n",
    "HOG_validation_list_std = train_std_norm.transform(HOG_validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_validation = pca_HOG_std_2.transform(HOG_validation_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_validation_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_validation = std_scale_train_2.transform(HOG_PCA_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_validation)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldValidation_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-validation-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For test set\n",
    "\n",
    "<u><b> Remarks :</b></u> We use 4-fold cross validaiton. Then, we need also to compute each kind of features for test set.\n",
    "So, for the test set, we extract 4 sets of features for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('..//_inputs//_images_Zooscan//ZooScan-test_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the test set\n",
    "HOG_test_list = extract_hog(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization \n",
    "HOG_test_list_std = train_std_norm.transform(HOG_test_list)\n",
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_test = pca_HOG_std_2.transform(HOG_test_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_test_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_test = std_scale_train_2.transform(HOG_PCA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_test)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfTest[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-test-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. For fold 2\n",
    "#### 1. Read path of fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFoldTraining_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-training-fold_2.csv')\n",
    "dfFoldValidation_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-validation-fold_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting HOG feature for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOG_list = extract_hog(dfFoldTraining_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard deviation normalization for later uses\n",
    "train_std_norm = StandardScaler().fit(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation normalization\n",
    "HOG_list_std = train_std_norm.transform(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HOG_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total HOG features:',(HOG_list_std.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PCA analysis on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_HOG_std = PCA().fit(HOG_list_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Plot PCA components and CEV\n",
    "\n",
    "From this, we can know number of components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100, linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Cumulative explained variance');\n",
    "\n",
    "ax.set_yticks(np.arange(0,105,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.axhline(y=90, linewidth=3, color='g', alpha=0.5)\n",
    "\n",
    "# ax.plot(800, 91, marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"green\")\n",
    "\n",
    "ax.set_title(\"PCA analysis on HOG features of training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100)[1100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remarks: More than 90% of variance is explained by first 1100 components</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit PCA to the HOG features\n",
    "\n",
    "1. We fit PCA (with n_components to keep) onto the training set\n",
    "2. Transform the training set with that PCA\n",
    "3. Use that PCA to transform the validation & test set\n",
    "\n",
    "##### 4.1. For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 1100 components which contribute to > 90 %\n",
    "pca_HOG_std_2 = PCA(n_components=1100)\n",
    "pca_HOG_std_2.fit(HOG_list_std)\n",
    "HOG_PCA_train = pca_HOG_std_2.transform(HOG_list_std)\n",
    "print(\"Original shape:   \", HOG_list_std.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG for train set --- standardization again\n",
    "std_scale_train_2 = preprocessing.StandardScaler().fit(HOG_PCA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in file\n",
    "X_HOG_std_train = std_scale_train_2.transform(HOG_PCA_train)\n",
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_train)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldTraining_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-train-fold_2.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the validation set\n",
    "HOG_validation_list = extract_hog(dfFoldValidation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization\n",
    "HOG_validation_list_std = train_std_norm.transform(HOG_validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_validation = pca_HOG_std_2.transform(HOG_validation_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_validation_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_validation = std_scale_train_2.transform(HOG_PCA_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_validation)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldValidation_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-validation-fold_2.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For test set\n",
    "\n",
    "<u><b> Remarks :</b></u> We use 4-fold cross validaiton. Then, we need also to compute each kind of features for test set.\n",
    "So, for the test set, we extract 4 sets of features for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('..//_inputs//_images_Zooscan//ZooScan-test_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the test set\n",
    "HOG_test_list = extract_hog(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization \n",
    "HOG_test_list_std = train_std_norm.transform(HOG_test_list)\n",
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_test = pca_HOG_std_2.transform(HOG_test_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_test_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_test = std_scale_train_2.transform(HOG_PCA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_test)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfTest[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-test-fold_2.csv\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
