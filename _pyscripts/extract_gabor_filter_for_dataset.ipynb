{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import skimage\n",
    "from skimage import io\n",
    "from skimage.feature import daisy, hog, ORB, local_binary_pattern, SIFT\n",
    "from skimage.color import label2rgb, rgb2gray\n",
    "from skimage.transform import resize, rotate, downscale_local_mean\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from skimage import exposure\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed, parallel_backend, cpu_count\n",
    "import psutil\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "\n",
    "import gabor_filters\n",
    "from  gabor_filters import gabor_filter\n",
    "from  gabor_filters import gabor_filter_response\n",
    "\n",
    "import importlib\n",
    "importlib.reload(gabor_filters)\n",
    "importlib.reload(gabor_filters.gabor_filter)\n",
    "importlib.reload(gabor_filters.gabor_filter_response)\n",
    "\n",
    "from gabor_filters.gabor_filter import GaborFilterBank as gbb\n",
    "from gabor_filters.gabor_filter_response import GaborFilteredResponseBank as gbfrb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.16\n",
      "0.19.2\n"
     ]
    }
   ],
   "source": [
    "print(python_version())\n",
    "print(skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_from_filepath(filepath):\n",
    "    # read image from its path\n",
    "    img = io.imread(filepath, as_gray=True)\n",
    "    \n",
    "    # Create Gabor filter bank\n",
    "    fmax = 0.327 # maximum frequency\n",
    "    k = np.sqrt(2) #frequency ratio or factor for selecting filter frequencies\n",
    "    p = 0.5 # crossing point between two consecutive filters, default 0.5\n",
    "    u = 6 #number of frequencies\n",
    "    v = 8 #number of orientation\n",
    "    gamma = 0.5  #smoothting parameter \n",
    "    eta = 0.5  #smoothting parameter of\n",
    "    row = img.shape[0]\n",
    "    col = img.shape[1] # size of image\n",
    "\n",
    "    GaborFilterBank = gbb().create_a_set_of_gabor_filters(fmax, k, p, u, v, row, col, gamma, eta)\n",
    "    \n",
    "    # Filter with the filter bank\n",
    "    GaborFilteredReponses = gbfrb().create_a_set_of_Gabor_filtered_responses(img, GaborFilterBank)\n",
    "\n",
    "    # Convert responses to simple 3-D matrix with normalization\n",
    "    filteredImages = gbfrb().convert_a_set_Gabor_filtered_responses_to_ndarray(GaborFilteredReponses)\n",
    "    \n",
    "    # Get mean and standard deviation of each response as Gabor (texture) features of an input image\n",
    "    nImages = filteredImages.shape[2]\n",
    "    textureFeatures = np.zeros(nImages*2)\n",
    "\n",
    "    index=0\n",
    "    for i in range(0, nImages):\n",
    "        textureFeatures[index] = np.mean(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "        textureFeatures[index] = np.std(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "    \n",
    "    return textureFeatures\n",
    "\n",
    "# def extract_gabor(dfDataset):\n",
    "#     # chunk the large dataset int smaller pieces\n",
    "#     n = 1000\n",
    "#     list_dfDataset_chunk = [dfDataset[i:i+n] for i in range(0, len(dfDataset), n)]\n",
    "    \n",
    "#     gabor_list = [None]*len(list_dfDataset_chunk)\n",
    "\n",
    "#     with parallel_backend(\"loky\", inner_max_num_threads=2):\n",
    "#         with Parallel(n_jobs=10, require='sharedmem') as parallel:\n",
    "#             for i, dfDataset_chunk in enumerate(tqdm(list_dfDataset_chunk, desc='Processing data', colour='blue', position=0, leave=True)):\n",
    "            \n",
    "#                 gabor_features = parallel(\n",
    "#                                 delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(dfDataset_chunk['filenames'], desc='Extract Gabor', colour='cyan', position=1, leave=False)\n",
    "#                             )\n",
    "            \n",
    "#                 gabor_list[i] = gabor_features\n",
    "\n",
    "#                 del  gabor_features\n",
    "#                 gc.collect()\n",
    "    \n",
    "#     return gabor_list\n",
    "\n",
    "\n",
    "# def extract_gabor_2(dfDataset):\n",
    "    \n",
    "#     # with parallel_backend(\"loky\", inner_max_num_threads=3):\n",
    "#     with Parallel(n_jobs=30, require='sharedmem', return_generator=True) as parallel:\n",
    "        \n",
    "#         gabor_features = parallel(\n",
    "#                         delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(dfDataset['filenames'], desc='Extract Gabor', colour='cyan')\n",
    "#                     )\n",
    "            \n",
    "#     return gabor_features\n",
    "\n",
    "\n",
    "# def extract_gabor_3(dfDataset):\n",
    "#     with Parallel(n_jobs=20, max_nbytes=100e6) as parallel:\n",
    "        \n",
    "#         gabor_features = parallel(\n",
    "#                         delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(dfDataset['filenames'], desc='Extract Gabor', colour='cyan')\n",
    "#                     )\n",
    "            \n",
    "#     return gabor_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_gabor_from_filepaths(filepaths):\n",
    "#     # Create ndarray of zeros to hold results\n",
    "#     n_filepaths = len(filepaths)\n",
    "#     n_features = 96\n",
    "#     textureFeatures_array = np.zeros((n_filepaths, n_features))\n",
    "    \n",
    "#     # Fill in results using Parallel\n",
    "#     for i, textureFeatures in enumerate(Parallel(n_jobs=20, backend='loky')(delayed(extract_gabor_from_filepath)(filepath) for filepath in tqdm(filepaths['filenames'], desc='Extract Gabor', colour='cyan'))):\n",
    "#         textureFeatures_array[i] = textureFeatures\n",
    "    \n",
    "#     return textureFeatures_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file(filepath):\n",
    "#     # call your function to extract texture features from file\n",
    "#     texture_features = extract_gabor_from_filepath(filepath)\n",
    "#     return texture_features\n",
    "\n",
    "# def process_files(filepaths):\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         results = executor.map(process_file, filepaths)\n",
    "#     return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_file_batch(filepaths):\n",
    "#     # results = []\n",
    "\n",
    "#     # Create ndarray of zeros to hold results\n",
    "#     n_filepaths = len(filepaths)\n",
    "#     n_features = 96\n",
    "#     results = np.zeros((n_filepaths, n_features))\n",
    "\n",
    "#     for i, filepath in enumerate(filepaths):\n",
    "#         # call your function to extract texture features from file\n",
    "#         results[i] = extract_gabor_from_filepath(filepath)\n",
    "#         # results.append(texture_features)\n",
    "#     return results\n",
    "\n",
    "# def process_files(filepaths, batch_size=10, n_jobs=-10):\n",
    "#     n_files = len(filepaths)\n",
    "#     batch_indices = np.arange(0, n_files, batch_size)\n",
    "#     if batch_indices[-1] != n_files:\n",
    "#         batch_indices = np.concatenate([batch_indices, [n_files]])\n",
    "#     results = []\n",
    "#     with tqdm(total=n_files) as pbar:\n",
    "#         for i in range(len(batch_indices) - 1):\n",
    "#             batch_filepaths = filepaths[batch_indices[i]:batch_indices[i+1]]\n",
    "#             batch_results = Parallel(n_jobs=n_jobs)(\n",
    "#                 delayed(process_file_batch)(batch_filepaths) for filepath in batch_filepaths)\n",
    "#             results.extend(batch_results)\n",
    "#             pbar.update(len(batch_filepaths))\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def process_files2(filepaths, batch_size=10, n_jobs=-1):\n",
    "#     n_files = len(filepaths)\n",
    "#     batch_indices = np.arange(0, n_files, batch_size)\n",
    "#     if batch_indices[-1] != n_files:\n",
    "#         batch_indices = np.concatenate([batch_indices, [n_files]])\n",
    "#     results = []\n",
    "#     with tqdm(total=n_files) as pbar:\n",
    "#         for i in range(len(batch_indices) - 1):\n",
    "#             batch_filepaths = filepaths[batch_indices[i]:batch_indices[i+1]]\n",
    "#             batch_results = Parallel(n_jobs=n_jobs)(\n",
    "#                 delayed(process_file_batch)(batch_filepaths))\n",
    "#             for batch_result in batch_results:\n",
    "#                 results.extend(batch_result)\n",
    "#             pbar.update(len(batch_filepaths))\n",
    "#     return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. main()\n",
    "\n",
    "### 4.2.1. For fold 1\n",
    "#### 1. Read path of fold 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFoldTraining_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-training-fold_1.csv')\n",
    "dfFoldValidation_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-validation-fold_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>labels</th>\n",
       "      <th>short_filenames</th>\n",
       "      <th>cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0001-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0003-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0004-aggregates.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..//_inputs//_images_Zooscan//_training//aggre...</td>\n",
       "      <td>aggregats_debris</td>\n",
       "      <td>0004.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filenames            labels  \\\n",
       "0  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "1  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "2  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "3  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "4  ..//_inputs//_images_Zooscan//_training//aggre...  aggregats_debris   \n",
       "\n",
       "       short_filenames  cls  \n",
       "0  0001-aggregates.png    0  \n",
       "1             0002.png    0  \n",
       "2  0003-aggregates.png    0  \n",
       "3  0004-aggregates.png    0  \n",
       "4             0004.png    0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(44099, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfFoldTraining_1.head(5), dfFoldTraining_1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting gabor feature for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(filepaths, batch_size=100, n_jobs=-1):\n",
    "    print(\"Iteration before chunking dataset: %0.3f MB\" % (\n",
    "                                        psutil.Process().memory_info().rss / 1e6))\n",
    "    n_files = len(filepaths)\n",
    "    batch_indices = np.arange(0, n_files, batch_size)\n",
    "    if batch_indices[-1] != n_files:\n",
    "        batch_indices = np.concatenate([batch_indices, [n_files]])\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=n_files) as pbar:\n",
    "        with Parallel(n_jobs=n_jobs, max_nbytes=1000e6) as parallel:\n",
    "            for i in range(len(batch_indices) - 1):\n",
    "                gc.collect()\n",
    "                batch_filepaths = filepaths[batch_indices[i]:batch_indices[i+1]]\n",
    "\n",
    "                print(\"Iteration %d: %0.3f MB\" % (\n",
    "                                        i, psutil.Process().memory_info().rss / 1e6))\n",
    "                batch_results = parallel(\n",
    "                                delayed(extract_gabor_from_filepath)(filepath) for filepath in batch_filepaths\n",
    "                            )\n",
    "            \n",
    "                for batch_result in batch_results:\n",
    "                    results.extend(batch_result)\n",
    "\n",
    "                print(\"Iteration %d after extending output: %0.3f MB\" % (i, psutil.Process().memory_info().rss / 1e6))\n",
    "                pbar.update(len(batch_filepaths))\n",
    "  \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration before chunking dataset: 255.189 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2558a4b17542009819fde033c17d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: 255.189 MB\n",
      "Iteration 0 after extending output: 256.090 MB\n",
      "Iteration 1: 256.102 MB\n",
      "Iteration 1 after extending output: 257.040 MB\n",
      "Iteration 2: 257.049 MB\n",
      "Iteration 2 after extending output: 257.503 MB\n",
      "Iteration 3: 257.622 MB\n",
      "Iteration 3 after extending output: 258.208 MB\n",
      "Iteration 4: 258.208 MB\n",
      "Iteration 4 after extending output: 258.937 MB\n",
      "Iteration 5: 258.945 MB\n",
      "Iteration 5 after extending output: 259.596 MB\n",
      "Iteration 6: 259.604 MB\n",
      "Iteration 6 after extending output: 260.239 MB\n",
      "Iteration 7: 260.252 MB\n",
      "Iteration 7 after extending output: 260.858 MB\n",
      "Iteration 8: 260.858 MB\n",
      "Iteration 8 after extending output: 261.825 MB\n",
      "Iteration 9: 261.837 MB\n",
      "Iteration 9 after extending output: 262.406 MB\n",
      "Iteration 10: 262.406 MB\n",
      "Iteration 10 after extending output: 263.381 MB\n",
      "Iteration 11: 263.393 MB\n",
      "Iteration 11 after extending output: 263.991 MB\n",
      "Iteration 12: 263.991 MB\n",
      "Iteration 12 after extending output: 264.942 MB\n",
      "Iteration 13: 264.950 MB\n",
      "Iteration 13 after extending output: 265.720 MB\n",
      "Iteration 14: 265.728 MB\n",
      "Iteration 14 after extending output: 266.453 MB\n",
      "Iteration 15: 266.465 MB\n",
      "Iteration 15 after extending output: 267.284 MB\n",
      "Iteration 16: 267.293 MB\n",
      "Iteration 16 after extending output: 269.672 MB\n",
      "Iteration 17: 269.672 MB\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m texture_features \u001b[39m=\u001b[39m process_files(dfFoldTraining_1[\u001b[39m'\u001b[39;49m\u001b[39mfilenames\u001b[39;49m\u001b[39m'\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mprocess_files\u001b[0;34m(filepaths, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m     15\u001b[0m batch_filepaths \u001b[39m=\u001b[39m filepaths[batch_indices[i]:batch_indices[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%0.3f\u001b[39;00m\u001b[39m MB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m     18\u001b[0m                         i, psutil\u001b[39m.\u001b[39mProcess()\u001b[39m.\u001b[39mmemory_info()\u001b[39m.\u001b[39mrss \u001b[39m/\u001b[39m \u001b[39m1e6\u001b[39m))\n\u001b[0;32m---> 19\u001b[0m batch_results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m     20\u001b[0m                 delayed(extract_gabor_from_filepath)(filepath) \u001b[39mfor\u001b[39;49;00m filepath \u001b[39min\u001b[39;49;00m batch_filepaths\n\u001b[1;32m     21\u001b[0m             )\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m batch_result \u001b[39min\u001b[39;00m batch_results:\n\u001b[1;32m     24\u001b[0m     results\u001b[39m.\u001b[39mextend(batch_result)\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "texture_features = process_files(dfFoldTraining_1['filenames'], batch_size=200, n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gabor_from_filepath(img):    \n",
    "    # Create Gabor filter bank\n",
    "    fmax = 0.327 # maximum frequency\n",
    "    k = np.sqrt(2) #frequency ratio or factor for selecting filter frequencies\n",
    "    p = 0.5 # crossing point between two consecutive filters, default 0.5\n",
    "    u = 6 #number of frequencies\n",
    "    v = 8 #number of orientation\n",
    "    gamma = 0.5  #smoothting parameter \n",
    "    eta = 0.5  #smoothting parameter of\n",
    "    row = img.shape[0]\n",
    "    col = img.shape[1] # size of image\n",
    "\n",
    "    GaborFilterBank = gbb().create_a_set_of_gabor_filters(fmax, k, p, u, v, row, col, gamma, eta)\n",
    "    \n",
    "    # Filter with the filter bank\n",
    "    GaborFilteredReponses = gbfrb().create_a_set_of_Gabor_filtered_responses(img, GaborFilterBank)\n",
    "\n",
    "    # Convert responses to simple 3-D matrix with normalization\n",
    "    filteredImages = gbfrb().convert_a_set_Gabor_filtered_responses_to_ndarray(GaborFilteredReponses)\n",
    "    \n",
    "    # Get mean and standard deviation of each response as Gabor (texture) features of an input image\n",
    "    nImages = filteredImages.shape[2]\n",
    "    textureFeatures = np.zeros(nImages*2)\n",
    "\n",
    "    index=0\n",
    "    for i in range(0, nImages):\n",
    "        textureFeatures[index] = np.mean(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "        textureFeatures[index] = np.std(np.abs(filteredImages[:,:,i]));\n",
    "        index = index + 1;\n",
    "    \n",
    "    return textureFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from more_itertools import chunked\n",
    "\n",
    "def image_generator(filepaths):\n",
    "    for filepath in filepaths:\n",
    "        yield io.imread(filepath, as_gray=True)\n",
    "\n",
    "def extract_texture_features(img):\n",
    "    textureFeatures = extract_gabor_from_filepath(img)\n",
    "    return textureFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da64c06a5346cb9984b977ab55dfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    852\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m results \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(chunked(image_generator(filepaths), batch_size), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(filepaths)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size):\n\u001b[0;32m----> 7\u001b[0m     batch_results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(pool\u001b[39m.\u001b[39;49mimap_unordered(extract_texture_features, batch, chunksize\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m      8\u001b[0m     results\u001b[39m.\u001b[39mextend(batch_results)\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 856\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepaths = dfFoldTraining_1['filenames']\n",
    "batch_size = 100\n",
    "num_processes = mp.cpu_count()-10\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    results = []\n",
    "    for batch in tqdm(chunked(image_generator(filepaths), batch_size), total=len(filepaths)//batch_size):\n",
    "        batch_results = list(pool.imap(extract_texture_features, batch, chunksize=1))\n",
    "        results.extend(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5889b16ceb4506a57eedc8315f110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepaths = dfFoldTraining_1['filenames']\n",
    "batch_size = 100\n",
    "chunk_size = 400\n",
    "num_processes = mp.cpu_count()-10\n",
    "with mp.Pool(processes=num_processes) as pool:\n",
    "    results = []\n",
    "    for batch in tqdm(chunked(image_generator(filepaths), batch_size), total=len(filepaths)//batch_size):\n",
    "        batch_results = list(pool.imap(extract_texture_features, batch, chunksize=chunk_size))\n",
    "        results.extend(batch_results)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "texture_features = process_files(dfFoldTraining_1['filenames'], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<multiprocessing.pool.ApplyResult at 0x7f5c7ba8b2e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texture_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gabor_train = process_files(dfFoldTraining_1['filenames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32592e079c44d60987f4ae303a185d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract Gabor:   0%|          | 0/44099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "exception calling callback for <Future at 0x7fa5e196e070 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/_base.py\", line 26, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py\", line 385, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py\", line 834, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 556, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/reusable_executor.py\", line 176, in submit\n",
      "    return super().submit(fn, *args, **kwargs)\n",
      "  File \"/home/centuri-mep-01/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 1129, in submit\n",
      "    raise self._flags.broken\n",
      "joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n",
      "\n",
      "The exit codes of the workers are {SIGKILL(-9)}\n"
     ]
    },
    {
     "ename": "TerminatedWorkerError",
     "evalue": "A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m list_gabor_train \u001b[39m=\u001b[39m extract_gabor_from_filepaths(dfFoldTraining_1)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mextract_gabor_from_filepaths\u001b[0;34m(filepaths)\u001b[0m\n\u001b[1;32m      5\u001b[0m textureFeatures_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n_filepaths, n_features))\n\u001b[1;32m      7\u001b[0m \u001b[39m# Fill in results using Parallel\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m i, textureFeatures \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mloky\u001b[39;49m\u001b[39m'\u001b[39;49m)(delayed(extract_gabor_from_filepath)(filepath) \u001b[39mfor\u001b[39;49;00m filepath \u001b[39min\u001b[39;49;00m tqdm(filepaths[\u001b[39m'\u001b[39;49m\u001b[39mfilenames\u001b[39;49m\u001b[39m'\u001b[39;49m], desc\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mExtract Gabor\u001b[39;49m\u001b[39m'\u001b[39;49m, colour\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcyan\u001b[39;49m\u001b[39m'\u001b[39;49m))):\n\u001b[1;32m      9\u001b[0m     textureFeatures_array[i] \u001b[39m=\u001b[39m textureFeatures\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m textureFeatures_array\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/_base.py:26\u001b[0m, in \u001b[0;36mFuture._invoke_callbacks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_done_callbacks:\n\u001b[1;32m     25\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         callback(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     27\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         LOGGER\u001b[39m.\u001b[39mexception(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexception calling callback for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:385\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.__call__\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    384\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel\u001b[39m.\u001b[39;49mdispatch_next()\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:834\u001b[0m, in \u001b[0;36mParallel.dispatch_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdispatch_next\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    827\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Dispatch more data for parallel processing\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \n\u001b[1;32m    829\u001b[0m \u001b[39m    This method is meant to be called concurrently by the multiprocessing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m \n\u001b[1;32m    833\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_original_iterator):\n\u001b[1;32m    835\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    836\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/_parallel_backends.py:556\u001b[0m, in \u001b[0;36mLokyBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    555\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_workers\u001b[39m.\u001b[39;49msubmit(SafeFunction(func))\n\u001b[1;32m    557\u001b[0m     future\u001b[39m.\u001b[39mget \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_future_result, future)\n\u001b[1;32m    558\u001b[0m     \u001b[39mif\u001b[39;00m callback \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/reusable_executor.py:176\u001b[0m, in \u001b[0;36m_ReusablePoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msubmit\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    175\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_submit_resize_lock:\n\u001b[0;32m--> 176\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msubmit(fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/image_processing/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:1129\u001b[0m, in \u001b[0;36mProcessPoolExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags\u001b[39m.\u001b[39mshutdown_lock:\n\u001b[1;32m   1128\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags\u001b[39m.\u001b[39mbroken \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1129\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags\u001b[39m.\u001b[39mbroken\n\u001b[1;32m   1130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flags\u001b[39m.\u001b[39mshutdown:\n\u001b[1;32m   1131\u001b[0m         \u001b[39mraise\u001b[39;00m ShutdownExecutorError(\n\u001b[1;32m   1132\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcannot schedule new futures after shutdown\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTerminatedWorkerError\u001b[0m: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n\nThe exit codes of the workers are {SIGKILL(-9)}"
     ]
    }
   ],
   "source": [
    "list_gabor_train = extract_gabor_from_filepaths(dfFoldTraining_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gabor_train2 = np.vstack(list_gabor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 96)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_gabor_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del list_gabor_train2, list_gabor_train\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2000\n",
    "\n",
    "list_dfFoldTraining_1_chunked = [dfFoldTraining_1[i:i+n] for i in range(0, len(dfFoldTraining_1), n)]\n",
    "\n",
    "display(len(list_dfFoldTraining_1_chunked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=2) as parallel:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gabor_list_train = None\n",
    "\n",
    "pbar = tqdm(list_dfFoldTraining_1_chunked)\n",
    "\n",
    "for i, dfFoldTraining_1_chunked in enumerate(pbar):\n",
    "    pbar.set_description(f'Processing the chunked data {i+1}')\n",
    "    \n",
    "    gabor_list_train_chunked = extract_gabor(dfFoldTraining_1_chunked)\n",
    "\n",
    "    gabor_list_train = np.vstack(gabor_list_train_chunked)\n",
    "\n",
    "    del  gabor_list_train_chunked\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard deviation normalization for later uses\n",
    "train_std_norm = StandardScaler().fit(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation normalization\n",
    "HOG_list_std = train_std_norm.transform(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HOG_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total HOG features:',(HOG_list_std.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PCA analysis on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_HOG_std = PCA().fit(HOG_list_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Plot PCA components and CEV\n",
    "\n",
    "From this, we can know number of components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100, linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Cumulative explained variance');\n",
    "\n",
    "ax.set_yticks(np.arange(0,105,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.axhline(y=90, linewidth=3, color='g', alpha=0.5)\n",
    "\n",
    "# ax.plot(800, 91, marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"green\")\n",
    "\n",
    "ax.set_title(\"PCA analysis on HOG features of training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100)[1100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remarks: More than 90% of variance is explained by first 1100 components</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Kaiser's rule in statistics: Pick components which have eigenvalues >= 1 or 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(pca_HOG_std.explained_variance_, 'bo-', linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_yticks(np.arange(0,125,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Eigenvalue')\n",
    "plt.axhline(y=1, linewidth=1, color='g', alpha=0.5)\n",
    "plt.title('Scree Plot of PCA: Component Eigenvalues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEigenvalues \\n%s' %pca_HOG_std.explained_variance_)\n",
    "print('Eigenvectors \\n%s' %pca_HOG_std.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaiser's rule in statistics: Pick components which have eigenvalues >= 1 or 0.7\n",
    "a = pca_HOG_std.explained_variance_ >= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Only 309 components are significant and should be kept </b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit PCA to the HOG features\n",
    "\n",
    "1. We fit PCA (with n_components to keep) onto the training set\n",
    "2. Transform the training set with that PCA\n",
    "3. Use that PCA to transform the validation & test set\n",
    "\n",
    "##### 4.1. For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 1100 components which contribute to > 90 %\n",
    "pca_HOG_std_2 = PCA(n_components=1100)\n",
    "pca_HOG_std_2.fit(HOG_list_std)\n",
    "HOG_PCA_train = pca_HOG_std_2.transform(HOG_list_std)\n",
    "print(\"Original shape:   \", HOG_list_std.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG for train set --- standardization again\n",
    "std_scale_train_2 = preprocessing.StandardScaler().fit(HOG_PCA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in file\n",
    "X_HOG_std_train = std_scale_train_2.transform(HOG_PCA_train)\n",
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_train)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldTraining_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-train-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the validation set\n",
    "HOG_validation_list = extract_hog(dfFoldValidation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization\n",
    "HOG_validation_list_std = train_std_norm.transform(HOG_validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_validation = pca_HOG_std_2.transform(HOG_validation_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_validation_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_validation = std_scale_train_2.transform(HOG_PCA_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_validation)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldValidation_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-validation-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For test set\n",
    "\n",
    "<u><b> Remarks :</b></u> We use 4-fold cross validaiton. Then, we need also to compute each kind of features for test set.\n",
    "So, for the test set, we extract 4 sets of features for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('..//_inputs//_images_Zooscan//ZooScan-test_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the test set\n",
    "HOG_test_list = extract_hog(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization \n",
    "HOG_test_list_std = train_std_norm.transform(HOG_test_list)\n",
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_test = pca_HOG_std_2.transform(HOG_test_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_test_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_test = std_scale_train_2.transform(HOG_PCA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_test)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfTest[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-test-fold_1.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. For fold 2\n",
    "#### 1. Read path of fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFoldTraining_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-training-fold_2.csv')\n",
    "dfFoldValidation_1 = pd.read_csv('..//_inputs//_images_Zooscan//_Zooscan-validation-fold_2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extracting HOG feature for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOG_list = extract_hog(dfFoldTraining_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard deviation normalization for later uses\n",
    "train_std_norm = StandardScaler().fit(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation normalization\n",
    "HOG_list_std = train_std_norm.transform(HOG_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HOG_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total HOG features:',(HOG_list_std.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. PCA analysis on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_HOG_std = PCA().fit(HOG_list_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Plot PCA components and CEV\n",
    "\n",
    "From this, we can know number of components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "ax.plot(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100, linewidth=2)\n",
    "ax.grid(color='r', linestyle='--', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Number of components')\n",
    "ax.set_ylabel('Cumulative explained variance');\n",
    "\n",
    "ax.set_yticks(np.arange(0,105,5))\n",
    "ax.set_xticks(np.arange(0,HOG_list.shape[1],100))\n",
    "\n",
    "ax.axhline(y=90, linewidth=3, color='g', alpha=0.5)\n",
    "\n",
    "# ax.plot(800, 91, marker=\"o\", markersize=10, markeredgecolor=\"red\", markerfacecolor=\"green\")\n",
    "\n",
    "ax.set_title(\"PCA analysis on HOG features of training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cumsum(pca_HOG_std.explained_variance_ratio_)*100)[1100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remarks: More than 90% of variance is explained by first 1100 components</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fit PCA to the HOG features\n",
    "\n",
    "1. We fit PCA (with n_components to keep) onto the training set\n",
    "2. Transform the training set with that PCA\n",
    "3. Use that PCA to transform the validation & test set\n",
    "\n",
    "##### 4.1. For training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep 1100 components which contribute to > 90 %\n",
    "pca_HOG_std_2 = PCA(n_components=1100)\n",
    "pca_HOG_std_2.fit(HOG_list_std)\n",
    "HOG_PCA_train = pca_HOG_std_2.transform(HOG_list_std)\n",
    "print(\"Original shape:   \", HOG_list_std.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOG for train set --- standardization again\n",
    "std_scale_train_2 = preprocessing.StandardScaler().fit(HOG_PCA_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in file\n",
    "X_HOG_std_train = std_scale_train_2.transform(HOG_PCA_train)\n",
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_train)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldTraining_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-train-fold_2.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the validation set\n",
    "HOG_validation_list = extract_hog(dfFoldValidation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization\n",
    "HOG_validation_list_std = train_std_norm.transform(HOG_validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_validation = pca_HOG_std_2.transform(HOG_validation_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_validation_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_validation = std_scale_train_2.transform(HOG_PCA_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_validation)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfFoldValidation_1[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-validation-fold_2.csv\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. For test set\n",
    "\n",
    "<u><b> Remarks :</b></u> We use 4-fold cross validaiton. Then, we need also to compute each kind of features for test set.\n",
    "So, for the test set, we extract 4 sets of features for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.read_csv('..//_inputs//_images_Zooscan//ZooScan-test_img.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract HOG features for the test set\n",
    "HOG_test_list = extract_hog(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization \n",
    "HOG_test_list_std = train_std_norm.transform(HOG_test_list)\n",
    "# Transform the HOG features using above PCA fitting\n",
    "HOG_PCA_test = pca_HOG_std_2.transform(HOG_test_list_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original shape:   \", HOG_test_list.shape)\n",
    "print(\"Transformed shape:\", HOG_PCA_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation normalization using above std_scale_train = preprocessing.StandardScaler().fit(HOG_PCA_train)\n",
    "X_HOG_std_test = std_scale_train_2.transform(HOG_PCA_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HOG_train_dff = pd.DataFrame(data = X_HOG_std_test)\n",
    "X_HOG_train_df = pd.DataFrame(data = dfTest[\"short_filenames\"])\n",
    "\n",
    "X_HOG_train_df = pd.concat([X_HOG_train_df,X_HOG_train_dff], axis=1)\n",
    "X_HOG_train_df.columns = pd.RangeIndex(X_HOG_train_df.columns.size)\n",
    "\n",
    "display(X_HOG_train_df.head(5), X_HOG_train_df.shape)\n",
    "\n",
    "X_HOG_train_df.to_csv(\"..//_inputs//_image_features//new//X-HOG_std_PCA_1100_std-test-fold_2.csv\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
